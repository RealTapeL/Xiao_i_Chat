# SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning)
SWIFTæ”¯æŒ350+ LLMå’Œ100+ MLLMï¼ˆå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼‰çš„è®­ç»ƒ(é¢„è®­ç»ƒã€å¾®è°ƒã€å¯¹é½)ã€æ¨ç†ã€è¯„æµ‹å’Œéƒ¨ç½²ã€‚å¼€å‘è€…å¯ä»¥ç›´æ¥å°†æˆ‘ä»¬çš„æ¡†æ¶åº”ç”¨åˆ°è‡ªå·±çš„Researchå’Œç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå®ç°æ¨¡å‹è®­ç»ƒè¯„æµ‹åˆ°åº”ç”¨çš„å®Œæ•´é“¾è·¯ã€‚æˆ‘ä»¬é™¤æ”¯æŒäº†PEFTæä¾›çš„è½»é‡è®­ç»ƒæ–¹æ¡ˆå¤–ï¼Œä¹Ÿæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„Adaptersåº“ä»¥æ”¯æŒæœ€æ–°çš„è®­ç»ƒæŠ€æœ¯ï¼Œå¦‚NEFTuneã€LoRA+ã€LLaMA-PROç­‰ï¼Œè¿™ä¸ªé€‚é…å™¨åº“å¯ä»¥è„±ç¦»è®­ç»ƒè„šæœ¬ç›´æ¥ä½¿ç”¨åœ¨è‡ªå·±çš„è‡ªå®šæµç¨‹ä¸­ã€‚
ä¸ºæ–¹ä¾¿ä¸ç†Ÿæ‚‰æ·±åº¦å­¦ä¹ çš„ç”¨æˆ·ä½¿ç”¨ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªGradioçš„web-uiç”¨äºæ§åˆ¶è®­ç»ƒå’Œæ¨ç†ï¼Œå¹¶æä¾›äº†é…å¥—çš„æ·±åº¦å­¦ä¹ è¯¾ç¨‹å’Œæœ€ä½³å®è·µä¾›æ–°æ‰‹å…¥é—¨ã€‚ 

ç°åœ¨æˆ‘ä»¬é¡¹ç›®ä½¿ç”¨æœ¬é¡¹ç›®è‡ªå®šä¹‰[æ•°æ®é›†](https://github.com/RealTapeL/Xiao_i_Chat/tree/main/data)ï¼Œå¹¶å°†å…¶è½¬åŒ–æˆåˆé€‚çš„Alpacaæ ¼å¼ï¼Œä½¿ç”¨SWIFTè¿›è¡Œå¾®è°ƒ.

## Swiftå¾®è°ƒæ¡†æ¶çš„å®‰è£…ä¸ä½¿ç”¨
### ç¯å¢ƒå‡†å¤‡
GPUè®¾å¤‡: 4090, 3090, A100ç­‰æ˜¾å­˜>24Gçš„æ˜¾å¡å‡å¯.

é¡¹ç›®Swiftå¾®è°ƒä½¿ç”¨é­”æ­ç¤¾åŒºæä¾›çš„åŸºäºè‹±ç‰¹å°”CPUçš„å…è´¹è®¡ç®—èµ„æºï¼Œä½¿ç”¨GPUç¯å¢ƒï¼ˆ8æ ¸ 32GB æ˜¾å­˜24Gï¼‰ï¼›

SWIFTåœ¨Pythonç¯å¢ƒä¸­è¿è¡Œã€‚è¯·ç¡®ä¿æ‚¨çš„Pythonç‰ˆæœ¬é«˜äº3.8ã€‚

è¿™é‡Œæˆ‘ä»¬å¯¹å®éªŒç¯å¢ƒè¿›è¡Œå®‰è£…ï¼Œå…¶ä¸­åŒ…å«äº†è™šæ‹Ÿç¯å¢ƒçš„åˆ›å»ºã€ms-swiftä»¥åŠç›¸å…³ä¾èµ–çš„å®‰è£…ã€‚
```
# è®¾ç½®pipå…¨å±€é•œåƒ
pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/
# å®‰è£…ms-swift
git clone https://github.com/modelscope/swift.git
cd swift
pip install -e '.[llm]'

# å¦‚æœä½ æƒ³è¦ä½¿ç”¨deepspeed.
pip install deepspeed -U

# ç¯å¢ƒå¯¹é½ (é€šå¸¸ä¸éœ€è¦è¿è¡Œ. å¦‚æœä½ è¿è¡Œé”™è¯¯, å¯ä»¥è·‘ä¸‹é¢çš„ä»£ç , ä»“åº“ä½¿ç”¨æœ€æ–°ç¯å¢ƒæµ‹è¯•)
pip install -r requirements/framework.txt  -U
pip install -r requirements/llm.txt  -U
```
### å¾®è°ƒï¼Œå¯åŠ¨ï¼ğŸ¤¯ğŸ¤¯ğŸ¤¯
#### ä½¿ç”¨Pythonå¾®è°ƒ
```
# Experimental environment: A10, 3090, V100, ...
# 20GB GPU memory
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import torch

from swift.llm import (
    DatasetName, InferArguments, ModelType, SftArguments,
    infer_main, sft_main, app_ui_main
)

model_type = ModelType.qwen_7b_chat
sft_args = SftArguments(
    model_type=model_type,
    dataset=[f'{DatasetName.blossom_math_zh}#2000'],
    output_dir='output')
result = sft_main(sft_args)
best_model_checkpoint = result['best_model_checkpoint']
print(f'best_model_checkpoint: {best_model_checkpoint}')
torch.cuda.empty_cache()

infer_args = InferArguments(
    ckpt_dir=best_model_checkpoint,
    load_dataset_config=True)
# merge_lora(infer_args, device_map='cpu')
result = infer_main(infer_args)
torch.cuda.empty_cache()

app_ui_main(infer_args)
```

ä¸ºäº†é™ä½ä½¿ç”¨é—¨æ§›ï¼ŒSwiftè¿˜è´´å¿ƒçš„å¢åŠ äº†ç•Œé¢è®­ç»ƒæ¨ç†çš„æ–¹å¼ã€‚å¦å¤–è¿˜æœ‰shè„šæœ¬çš„ä½¿ç”¨æ–¹å¼,å¯ä»¥Githubä¸ŠæŸ¥é˜…swiftçš„[å®˜æ–¹æ–‡æ¡£](https://github.com/modelscope/ms-swift/blob/main/README_CN.md)å»äº†è§£ã€‚
#### ä½¿ç”¨CILå‘½ä»¤å¾®è°ƒ
```
# Experimental environment: A10, 3090, V100, ...
# 20GB GPU memory
CUDA_VISIBLE_DEVICES=0 swift sft \
    --model_id_or_path qwen/Qwen-7B-Chat \
    --dataset AI-ModelScope/blossom-math-v2 \
    --output_dir output \
    --model_type your model_type

# ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†
CUDA_VISIBLE_DEVICES=0 swift sft \
    --model_id_or_path qwen/Qwen-7B-Chat \
    --dataset chatml.jsonl \
    --output_dir output \
    --model_type your model_type

# ä½¿ç”¨DDP
# Experimental environment: 2 * 3090
# 2 * 23GB GPU memory
CUDA_VISIBLE_DEVICES=0,1 \
NPROC_PER_NODE=2 \
swift sft \
    --model_id_or_path qwen/Qwen-7B-Chat \
    --dataset AI-ModelScope/blossom-math-v2 \
    --output_dir output \

# å¤šæœºå¤šå¡
# node0
CUDA_VISIBLE_DEVICES=0,1,2,3 \
NNODES=2 \
NODE_RANK=0 \
MASTER_ADDR=127.0.0.1 \
NPROC_PER_NODE=4 \
swift sft \
    --model_id_or_path qwen/Qwen-7B-Chat \
    --dataset AI-ModelScope/blossom-math-v2 \
    --output_dir output \
# node1
CUDA_VISIBLE_DEVICES=0,1,2,3 \
NNODES=2 \
NODE_RANK=1 \
MASTER_ADDR=xxx.xxx.xxx.xxx \
NPROC_PER_NODE=4 \
swift sft \
    --model_id_or_path qwen/Qwen-7B-Chat \
    --dataset AI-ModelScope/blossom-math-v2 \
    --output_dir output \
Â·Â·Â·
